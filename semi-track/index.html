<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Learning to Track Instances without Video Annotations">
    <meta name="author" content="Yang Fu,
                                Sifei Liu,
                                Umar Iqbal,
                                Shalini De Mello,
				Humphrey Shi,
                                Jan Kautz">

    <title>Learning to Track Instances without Video Annotations</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Learning to Track Instances without Video Annotations</h2>
    <h3>CVPR 2021 (Oral)</h3>
<!--            <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>-->
    <hr>
    <p class="authors">
        <a href="https://oasisyang.github.io"> Yang Fu</a>,
        <a href="https://www.sifeiliu.net"> Sifei Liu</a>,
	<a href="http://www.umariqbal.info"> Umar Iqbal</a>,
        <a href="https://research.nvidia.com/person/shalini-gupta"> Shalini De Mello</a>,</br>
        <a href="https://www.humphreyshi.com"> Humphrey Shi</a>,
        <a href="https://jankautz.com"> Jan Kautz</a>,
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2104.00287">Paper</a>
        <a class="btn btn-primary" href="">Code(Soon)</a>
    </div>
</div>

<div class="container">
    <div class="section">
        <div class="vcontainer">
            <iframe class='video' src="https://www.youtube.com/embed/2CX0xCKqF8M" frameborder="0"
                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
        </div>
        <hr>
        <p>
            Tracking segmentation masks of multiple instances has been intensively studied, 
            but still faces two fundamental challenges: 1) the requirement of large-scale, 
            frame-wise annotation, and 2) the complexity of two-stage approaches. To resolve 
            these challenges, we introduce a novel semi-supervised framework by learning 
            instance tracking networks with only a labeled image dataset and unlabeled video 
            sequences. With an instance contrastive objective, we learn an embedding to 
            discriminate each instance from the others. We show that even when only trained 
            with images, the learned feature representation is robust to instance appearance 
            variations, and is thus able to track objects steadily across frames. We further 
            enhance the tracking capability of the embedding by learning correspondence from 
            unlabeled videos in a self-supervised manner. In addition, we integrate this 
            module into single-stage instance segmentation and pose estimation frameworks, 
            which significantly reduce the computational complexity of tracking compared to 
            two-stage networks. We conduct experiments on the YouTube-VIS and PoseTrack datasets. 
            Without any video annotation efforts, our proposed method can achieve comparable 
            or even better performance than most fully-supervised methods.
        </p>
    </div>

    <!-- <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href=""
                   class="list-group-item">
                    <img src="img/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div> -->

    <!-- <div class="section">
        <h2>Framework</h2>
            <img src="./assets/teaser.png" style="border-width: 0px;width: 100%;"  class="img-responsive">
    </div> -->

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @inproceedings{fu2021track,
                author = {Fu, Yang
                          and Liu, Sifei
			  and Iqbal, Umar
                          and De Mello, Shalini
                          and Shi, Humphrey
                          and Kautz, Jan},
                title = {Learning to Track Instances without Video Annotations},
                booktitle = {Proceedings of the IEEE conference on 
                    computer vision and pattern recognition},
                year={2021}
            }
        </div>
    </div>
    <hr>

    <footer>
        <p>Acknowledgment: website template from <a href="https://vsitzmann.github.io/siren/">SIREN</a></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
